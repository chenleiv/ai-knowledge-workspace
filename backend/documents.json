[
  {
    "title": "JWT Authentication Deep Dive",
    "category": "Security",
    "summary": "How JWT authentication works, common pitfalls, and security best practices.",
    "content": "JSON Web Tokens (JWT) are a compact, URL-safe way to represent claims between two parties. A JWT consists of three parts: header, payload, and signature, separated by dots.\n\nThe header specifies the signing algorithm (for example HS256 or RS256). The payload contains claims such as sub, exp, iat, and custom application data. The signature is created by signing the base64-encoded header and payload using a secret (HMAC) or a private key (RSA/ECDSA).\n\nJWTs are often used in stateless authentication systems where the server does not store session state. Instead, the token carries identity and authorization claims.\n\nSecurity considerations:\n- Always validate the signature, algorithm, issuer/audience (if used), and expiration (exp).\n- Avoid storing sensitive data in the payload because it is only encoded, not encrypted.\n- Use short-lived access tokens and refresh tokens for longer sessions.\n- Prefer RS256 (asymmetric) in distributed systems to avoid sharing a single secret across services.\n\nA common mistake is storing access tokens in localStorage, which increases the impact of XSS. A safer option is HttpOnly cookies with CSRF defenses (SameSite, CSRF token, and careful CORS settings).\n\nRevocation is another common pitfall. With stateless JWTs, immediate revocation is hard. Typical mitigations are short expirations, server-side refresh token revocation lists, and rotating refresh tokens.\n\nFinally, be careful with clock skew and token validation. Some systems apply a small leeway window to avoid rejecting tokens due to time drift, but do not make the leeway too large.",
    "id": 1
  },
  {
    "title": "REST vs GraphQL APIs",
    "category": "Backend",
    "summary": "Comparison between REST and GraphQL, including tradeoffs, caching, and abuse prevention.",
    "content": "REST APIs are resource-based and usually expose multiple endpoints like /users, /orders, or /products. Each endpoint typically returns a fixed response structure.\n\nGraphQL exposes a single endpoint and lets clients request exactly the fields they need. This can reduce over-fetching and under-fetching, especially in frontend-heavy applications.\n\nKey differences:\n- REST: multiple endpoints, fixed shapes, relies on HTTP semantics (status codes, caching headers).\n- GraphQL: single endpoint, flexible shapes, errors often returned in a standard errors array, and HTTP status is frequently 200 even when query-level errors exist.\n\nCaching:\nREST is easier to cache with standard HTTP caches (ETag, Cache-Control). GraphQL is harder to cache at the transport layer because many queries go to the same endpoint. GraphQL caching often moves to the client (normalized caches) or requires persisted queries.\n\nOperational complexity:\nGraphQL introduces schema design, resolvers, and query complexity management. Without limits, nested queries can become expensive. Mitigations include depth limits, complexity scoring, query whitelisting (persisted queries), and rate limiting.\n\nWhen REST is better:\n- Simple CRUD services\n- Strong reliance on HTTP caching\n- Teams preferring simpler operational tooling\n\nWhen GraphQL is better:\n- Multiple clients with different data needs\n- Rapid iteration on frontend requirements\n- Complex data graphs where composing multiple REST calls becomes painful",
    "id": 2
  },
  {
    "title": "React Rendering and Performance Optimization",
    "category": "Frontend",
    "summary": "How React renders components and practical techniques to reduce unnecessary re-renders.",
    "content": "React re-renders a component when its state changes, when its props change, or when its parent re-renders and passes new references. React uses a virtual DOM diff to compute minimal DOM updates.\n\nCommon performance problems come from unnecessary re-renders:\n- Inline object and function props cause new references each render.\n- Large lists re-render on every keystroke.\n- State stored too high in the tree causes many descendants to update.\n\nOptimizations:\n- React.memo: memoize pure components so they only re-render when props change.\n- useMemo: memoize expensive computations, derived lists, and filtered results.\n- useCallback: memoize functions passed down to children, preventing prop reference changes.\n\nList keys:\nStable keys are essential. Using array indexes as keys can cause incorrect UI behavior during insertions/reordering, and can break component state.\n\nState placement:\nStore state as close as possible to where it is used. For shared state, consider context, but be careful: context updates re-render all consumers. For large apps, selective state management patterns or splitting contexts helps.\n\nProfiling:\nUse React DevTools Profiler to identify slow commits and components that re-render too often. In many cases, the best optimization is simplifying data flow and reducing rendering work rather than premature memoization.\n\nPractical tip:\nOptimize only after measuring. Memoization adds complexity and can be counterproductive if dependencies change frequently.",
    "id": 3
  },
  {
    "title": "Docker Containers Explained",
    "category": "DevOps",
    "summary": "Docker fundamentals: images, containers, volumes, networks, and how it differs from virtual machines.",
    "content": "Docker runs applications in isolated environments called containers. A container packages the application with its dependencies and runtime configuration.\n\nContainers vs virtual machines:\n- Virtual machines include a full guest OS per VM.\n- Containers share the host OS kernel, making them smaller and faster to start.\n\nCore concepts:\n- Image: an immutable template built from layers (often from a Dockerfile).\n- Container: a running instance of an image.\n- Volume: persistent storage that lives outside the container lifecycle.\n- Network: allows containers to communicate, with DNS-based discovery in many setups.\n\nDockerfile basics:\nA Dockerfile describes how to build an image: base image, copying app files, installing dependencies, and defining the startup command.\n\nWhy teams use Docker:\n- Consistent environments across dev/staging/prod\n- Easier onboarding and reproducible builds\n- Works well with CI pipelines\n\nGotchas:\n- Containers still require security updates and monitoring.\n- Do not bake secrets into images.\n- Be mindful of file permissions, volume mounts, and host networking.\n\nFor local development, Docker Compose is often used to run multi-service setups (API, database, cache) with a single command.",
    "id": 4
  },
  {
    "title": "OAuth 2.0 Authorization Flows",
    "category": "Security",
    "summary": "OAuth flows overview with practical guidance for SPAs, web apps, and service-to-service communication.",
    "content": "OAuth 2.0 is an authorization framework that allows an application to access resources on behalf of a user without sharing the user's password. OAuth defines roles like Resource Owner, Client, Authorization Server, and Resource Server.\n\nCommon flows:\n- Authorization Code Flow: recommended for server-rendered web apps.\n- Authorization Code with PKCE: recommended for SPAs and mobile apps. PKCE mitigates authorization code interception.\n- Client Credentials Flow: service-to-service calls where no user is involved.\n\nImplicit Flow is deprecated for modern SPAs due to security concerns.\n\nImportant details:\n- Redirect URIs must be strict and validated.\n- Scopes should be minimal (least privilege).\n- Access tokens should be short-lived; refresh tokens require careful storage and rotation.\n\nOAuth vs Authentication:\nOAuth itself is not authentication. For authentication, OpenID Connect (OIDC) extends OAuth with an ID token and standard identity claims.\n\nCommon misconfigurations:\n- Overly broad scopes\n- Insecure redirect URI patterns\n- Weak token storage choices in the browser\n\nFor SPAs:\nUse PKCE + Authorization Code flow, store tokens carefully, and ensure CORS and cookie settings are correct if using HttpOnly cookies.",
    "id": 5
  },
  {
    "title": "Linux Process Management Basics",
    "category": "Linux",
    "summary": "Processes, signals, foreground/background jobs, and practical commands for managing running programs.",
    "content": "In Linux, a running program is a process identified by a PID (process ID). You can inspect processes using ps, top, or htop. Each process has a state (running, sleeping, stopped, zombie) and resource usage.\n\nSignals are a primary mechanism to control processes:\n- SIGTERM: request graceful shutdown (default for kill).\n- SIGKILL: immediate termination (cannot be caught).\n- SIGINT: interrupt (Ctrl+C) for foreground processes.\n\nJob control:\n- Run a command in the background by appending &: for example, \"myapp &\".\n- Suspend a foreground process with Ctrl+Z.\n- Resume with fg (foreground) or bg (background).\n\nPractical tips:\n- Prefer SIGTERM before SIGKILL to allow cleanup.\n- Use pgrep to find PIDs by name.\n- Use systemd service files for long-running daemons in production environments.\n\nUnderstanding processes and signals helps debug runaway CPU usage, memory leaks, and stuck programs, and is a core skill for troubleshooting servers.",
    "id": 6
  },
  {
    "title": "1",
    "category": "1",
    "summary": "1",
    "content": "1",
    "id": 7
  }
]